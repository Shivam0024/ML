# -*- coding: utf-8 -*-
"""Morph2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10x5n0T3kKL6XGM3-rwR_MaCLTOwRCL3C
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Create the .kaggle directory
!mkdir -p ~/.kaggle

# Move the kaggle.json file (adjust the path if needed)
!mv /content/kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2

import dlib
import cv2
import networkx as nx
import matplotlib.pyplot as plt
import os

# Initialize the dlib face detector and the facial landmark predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("/content/shape_predictor_68_face_landmarks.dat")

# Define the folder containing images
folder_path = "/content/images/"

# Define the selected landmark indices
selected_landmark_indices = [0, 1, 2, 7, 8, 9, 14, 15, 16, 17, 19, 21, 22, 24, 26, 27, 30, 31, 33, 35, 36, 37, 39, 40, 42, 44, 45, 47, 48, 51, 54, 57]

# Define edges between the selected nodes
edges = [
    (0, 1), (1, 2), (2, 7), (7, 8), (8, 9), (9, 14), (14, 15), (15, 16),
    (17, 19), (19, 21),
    (22, 24), (24, 26),
    (27, 30),
    (31, 33), (33, 35), (30, 33),
    (36, 37), (37, 39), (39, 40), (36, 40), (19, 37),
    (42, 44), (44, 45), (45, 47), (42, 47), (24, 44),
    (48, 51), (51, 54), (54, 57), (48, 57),
    (33, 51), (8, 57)
]

# Iterate over all images in the folder
for filename in os.listdir(folder_path):
    if filename.endswith((".jpg", ".jpeg", ".png")):
        image_path = os.path.join(folder_path, filename)

        # Read the input image
        img = cv2.imread(image_path)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # Detect face in the image
        faces = detector(gray)

        for face in faces:
            # Detect landmarks
            landmarks = predictor(gray, face)

            # Store detected landmarks in a dictionary with selected indices
            detected_landmarks = {i: (landmarks.part(i).x, landmarks.part(i).y) for i in selected_landmark_indices if i < landmarks.num_parts}

            # Create the graph with nodes as detected landmarks
            G = nx.Graph()

            # Add nodes
            for i, point in detected_landmarks.items():
                G.add_node(i, pos=point)

            # Add edges between selected landmarks
            for (i, j) in edges:
                if i in detected_landmarks and j in detected_landmarks:
                    G.add_edge(i, j)

            # Plot the graph over the image
            pos = nx.get_node_attributes(G, 'pos')
            nx.draw(G, pos, node_color='green', node_size=70, edge_color='blue', with_labels=True, font_size=8)

            # Display the graph and image
            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            plt.axis("off")
            plt.show()

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d chiragsaipanuganti/morph
# datasets/

!unzip morph.zip -d /content/morph

import dlib
import cv2
import numpy as np
from tqdm import tqdm
import networkx as nx
import matplotlib.pyplot as plt
import os

# Initialize the dlib face detector and the facial landmark predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("/content/shape_predictor_68_face_landmarks.dat")

# Define the folder containing images
folder_path = "/content/morph/Dataset/Images/Test"

# Define the selected landmark indices
selected_landmark_indices = [0, 1, 2, 7, 8, 9, 14, 15, 16, 17, 19, 21, 22, 24, 26, 27, 30, 31, 33, 35, 36, 37, 39, 40, 42, 44, 45, 47, 48, 51, 54, 57]

# Define edges between the selected nodes
edges = [
    (0, 1), (1, 2), (2, 7), (7, 8), (8, 9), (9, 14), (14, 15), (15, 16),
    (17, 19), (19, 21),
    (22, 24), (24, 26),
    (27, 30), (21,22),(17,0),(26,16),
    (31, 33), (33, 35), (30, 33),
    (36, 37), (37, 39), (39, 40), (36, 40), (19, 37),(27,21),(27,39),(27,31),(27,35),(27,22),(27,42),
    (42, 44), (44, 45), (45, 47), (42, 47), (24, 44),
    (48, 51), (51, 54), (54, 57), (48, 57),
    (33, 51), (8, 57)
]

# Iterate over all images in the folder
for filename in tqdm(os.listdir(folder_path)):
    if filename.endswith((".JPG", ".jpeg", ".png")):
        image_path = os.path.join(folder_path, filename)

        # Read the input image
        img = cv2.imread(image_path)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # Detect face in the image
        faces = detector(gray)

        for face in faces:
            # Detect landmarks
            landmarks = predictor(gray, face)

            # Store detected landmarks in a dictionary with selected indices
            detected_landmarks = {i: (landmarks.part(i).x, landmarks.part(i).y) for i in selected_landmark_indices if i < landmarks.num_parts}

            # Create the graph with nodes as detected landmarks
            G = nx.Graph()

            # Add nodes
            for i, point in detected_landmarks.items():
                G.add_node(i, pos=point)

            # Add edges between selected landmarks
            for (i, j) in edges:
                if i in detected_landmarks and j in detected_landmarks:
                    G.add_edge(i, j)

            # Create subplots for image and edge network
            fig, axs = plt.subplots(1, 2, figsize=(12, 6))

            # Plot the graph over the image
            pos = nx.get_node_attributes(G, 'pos')
            nx.draw(G, pos, node_color='green', node_size=70, edge_color='blue', with_labels=True, font_size=8, ax=axs[0])

            # Display the image with graph
            axs[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            axs[0].axis("off")
            axs[0].set_title('Graph Overlay')

            # Extract edge positions for plotting
            edge_positions = [(detected_landmarks[i], detected_landmarks[j]) for (i, j) in G.edges()]
            edge_positions = [coords for sublist in edge_positions for coords in sublist]
            edge_positions = np.array(edge_positions)

            # Plot the edge network
            axs[1].scatter(edge_positions[:, 0], edge_positions[:, 1], c='blue', s=50, label='Edges')
            for (i, j) in G.edges():
                x = [detected_landmarks[i][0], detected_landmarks[j][0]]
                y = [detected_landmarks[i][1], detected_landmarks[j][1]]
                axs[1].plot(x, y, 'r-')
            axs[1].invert_yaxis()
            axs[1].set_title('Edge Network')
            axs[1].set_xlabel('X coordinate')
            axs[1].set_ylabel('Y coordinate')

            plt.tight_layout()
            plt.show()

import dlib
import cv2
import numpy as np
from tqdm import tqdm
import networkx as nx
import matplotlib.pyplot as plt
import os

# Initialize the dlib face detector and the facial landmark predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("/content/shape_predictor_68_face_landmarks.dat")

# Define the folder containing images
folder_path = "/content/morph/Dataset/Images/Test"

# Define the selected landmark indices
selected_landmark_indices = [0, 1, 2, 7, 8, 9, 14, 15, 16, 17, 19, 21, 22, 24, 26, 27, 30, 31, 33, 35, 36, 37, 39, 40, 42, 44, 45, 47, 48, 51, 54, 57]

# Define edges between the selected nodes using f1, f2, ..., f32
edges = [
    ('f1', 'f2'), ('f2', 'f3'), ('f3', 'f4'), ('f4', 'f5'), ('f5', 'f6'), ('f6', 'f7'), ('f7', 'f8'), ('f8', 'f9'),
    ('f10', 'f11'), ('f11', 'f12'),
    ('f13', 'f14'), ('f14', 'f15'),
    ('f16', 'f17'),
    ('f18', 'f19'), ('f19', 'f20'), ('f17', 'f19'),
    ('f21', 'f22'), ('f22', 'f23'), ('f23', 'f24'), ('f21', 'f24'), ('f11', 'f22'), ('f16', 'f23'),
    ('f25', 'f26'), ('f26', 'f27'), ('f27', 'f28'), ('f25', 'f28'), ('f14', 'f26'), ('f16', 'f25'),
    ('f29', 'f30'), ('f30', 'f31'), ('f31', 'f32'), ('f29', 'f32'),
    ('f19', 'f30'), ('f5', 'f32')
]

# Map landmark indices to node names (f1, f2, ..., f32)
index_to_node = {index: f'f{idx+1}' for idx, index in enumerate(selected_landmark_indices)}

# Iterate over all images in the folder
for filename in tqdm(os.listdir(folder_path)):
    if filename.endswith((".JPG", ".jpeg", ".png")):
        image_path = os.path.join(folder_path, filename)

        # Read the input image
        img = cv2.imread(image_path)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # Detect face in the image
        faces = detector(gray)

        for face in faces:
            # Detect landmarks
            landmarks = predictor(gray, face)

            # Store detected landmarks in a dictionary with selected indices
            detected_landmarks = {i: (landmarks.part(i).x, landmarks.part(i).y) for i in selected_landmark_indices if i < landmarks.num_parts}

            # Map landmark indices to node names f1, f2, ..., f32
            landmark_coords = {index_to_node[i]: detected_landmarks[i] for i in selected_landmark_indices if i in detected_landmarks}

            # Create the graph with nodes as detected landmarks
            G = nx.Graph()

            # Add nodes
            for name, point in landmark_coords.items():
                G.add_node(name, pos=point)

            # Add edges between selected landmarks
            for (i, j) in edges:
                if i in landmark_coords and j in landmark_coords:
                    G.add_edge(i, j)

            # Create subplots for image and edge network
            fig, axs = plt.subplots(1, 2, figsize=(12, 6))

            # Plot the graph over the image
            pos = nx.get_node_attributes(G, 'pos')
            nx.draw(G, pos, node_color='green', node_size=70, edge_color='blue', with_labels=True, font_size=8, ax=axs[0])

            # Display the image with graph
            axs[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            axs[0].axis("off")
            axs[0].set_title('Graph Overlay')

            # Extract edge positions for plotting
            edge_positions = [(landmark_coords[i], landmark_coords[j]) for (i, j) in G.edges()]
            edge_positions = [coords for sublist in edge_positions for coords in sublist]
            edge_positions = np.array(edge_positions)

            # Plot the edge network
            axs[1].scatter(edge_positions[:, 0], edge_positions[:, 1], c='blue', s=50, label='Edges')
            for (i, j) in G.edges():
                x = [landmark_coords[i][0], landmark_coords[j][0]]
                y = [landmark_coords[i][1], landmark_coords[j][1]]
                axs[1].plot(x, y, 'r-')
            axs[1].invert_yaxis()
            axs[1].set_title('Edge Network')
            axs[1].set_xlabel('X coordinate')
            axs[1].set_ylabel('Y coordinate')

            plt.tight_layout()
            plt.show()

!pip install torch-geometric

import dlib
import cv2
import numpy as np
import networkx as nx
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
import torch
from torch_geometric.data import Data

# Initialize the dlib face detector and the facial landmark predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("/content/shape_predictor_68_face_landmarks.dat")

# Define the folder containing images
folder_path = "/content/morph/Dataset/Images/Test"

# Define the selected landmark indices
selected_landmark_indices = [0, 1, 2, 7, 8, 9, 14, 15, 16, 17, 19, 21, 22, 24, 26, 27, 30, 31, 33, 35, 36, 37, 39, 40, 42, 44, 45, 47, 48, 51, 54, 57]

# Define edges between the selected nodes using f1, f2, ..., f32
edges = [
    ('f1', 'f2'), ('f2', 'f3'), ('f3', 'f4'), ('f4', 'f5'), ('f5', 'f6'), ('f6', 'f7'), ('f7', 'f8'), ('f8', 'f9'),
    ('f10', 'f11'), ('f11', 'f12'),
    ('f13', 'f14'), ('f14', 'f15'),
    ('f16', 'f17'),
    ('f18', 'f19'), ('f19', 'f20'), ('f17', 'f19'),
    ('f21', 'f22'), ('f22', 'f23'), ('f23', 'f24'), ('f21', 'f24'), ('f11', 'f22'), ('f16', 'f23'),
    ('f25', 'f26'), ('f26', 'f27'), ('f27', 'f28'), ('f25', 'f28'), ('f14', 'f26'), ('f16', 'f25'),
    ('f29', 'f30'), ('f30', 'f31'), ('f31', 'f32'), ('f29', 'f32'),
    ('f19', 'f30'), ('f5', 'f32')
]

# Map landmark indices to node names (f1, f2, ..., f32)
index_to_node = {index: f'f{idx+1}' for idx, index in enumerate(selected_landmark_indices)}
node_to_index = {node: idx for idx, node in enumerate(index_to_node.values())}

# Prepare data for GNN
def create_graph_data():
    edge_index = []

    # Iterate over all images in the folder
    for filename in tqdm(os.listdir(folder_path)):
        if filename.endswith((".JPG", ".jpeg", ".png")):
            image_path = os.path.join(folder_path, filename)

            # Read the input image
            img = cv2.imread(image_path)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            # Detect face in the image
            faces = detector(gray)

            for face in faces:
                # Detect landmarks
                landmarks = predictor(gray, face)

                # Store detected landmarks in a dictionary with selected indices
                detected_landmarks = {i: (landmarks.part(i).x, landmarks.part(i).y) for i in selected_landmark_indices if i < landmarks.num_parts}

                # Map landmark indices to node names f1, f2, ..., f32
                landmark_coords = {index_to_node[i]: detected_landmarks[i] for i in selected_landmark_indices if i in detected_landmarks}

                # Create the graph with nodes as detected landmarks
                G = nx.Graph()

                # Add nodes
                for name, point in landmark_coords.items():
                    G.add_node(name, pos=point)

                # Add edges between selected landmarks
                for (i, j) in edges:
                    if i in landmark_coords and j in landmark_coords:
                        G.add_edge(i, j)

                # Convert NetworkX graph to PyTorch Geometric format
                pos = nx.get_node_attributes(G, 'pos')
                x = torch.tensor([pos[name] for name in G.nodes()], dtype=torch.float)

                # Create edge index tensor
                edge_list = list(G.edges())
                edge_index = torch.tensor([[node_to_index[edge[0]] for edge in edge_list],
                                           [node_to_index[edge[1]] for edge in edge_list]], dtype=torch.long)

                data = Data(x=x, edge_index=edge_index)

                # Visualize the graph if needed
                fig, axs = plt.subplots(1, 2, figsize=(12, 6))

                # Plot the graph over the image
                nx.draw(G, pos, node_color='green', node_size=70, edge_color='blue', with_labels=True, font_size=8, ax=axs[0])

                # Display the image with graph
                axs[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                axs[0].axis("off")
                axs[0].set_title('Graph Overlay')

                # Extract edge positions for plotting
                edge_positions = [(landmark_coords[i], landmark_coords[j]) for (i, j) in G.edges()]
                edge_positions = [coords for sublist in edge_positions for coords in sublist]
                edge_positions = np.array(edge_positions)

                # Plot the edge network
                axs[1].scatter(edge_positions[:, 0], edge_positions[:, 1], c='blue', s=50, label='Edges')
                for (i, j) in G.edges():
                    x = [landmark_coords[i][0], landmark_coords[j][0]]
                    y = [landmark_coords[i][1], landmark_coords[j][1]]
                    axs[1].plot(x, y, 'r-')
                axs[1].invert_yaxis()
                axs[1].set_title('Edge Network')
                axs[1].set_xlabel('X coordinate')
                axs[1].set_ylabel('Y coordinate')

                plt.tight_layout()
                plt.show()

                return data

# Create graph data
graph_data = create_graph_data()

# Now `graph_data` can be used for training/testing your GNN

import dlib
import cv2
import numpy as np
import networkx as nx
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
import torch
from torch_geometric.data import Data

# Initialize the dlib face detector and the facial landmark predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("/content/shape_predictor_68_face_landmarks.dat")

# Define the folder containing images
folder_path = "/content/morph/Dataset/Images/Test"

# Define the selected landmark indices
selected_landmark_indices = [0, 1, 2, 7, 8, 9, 14, 15, 16, 17, 19, 21, 22, 24, 26, 27, 30, 31, 33, 35, 36, 37, 39, 40, 42, 44, 45, 47, 48, 51, 54, 57]

# Define edges between the selected nodes using f1, f2, ..., f32
edges = [
    ('f1', 'f2'), ('f2', 'f3'), ('f3', 'f4'), ('f4', 'f5'), ('f5', 'f6'), ('f6', 'f7'), ('f7', 'f8'), ('f8', 'f9'),
    ('f10', 'f11'), ('f11', 'f12'),
    ('f13', 'f14'), ('f14', 'f15'),
    ('f16', 'f17'),
    ('f18', 'f19'), ('f19', 'f20'), ('f17', 'f19'),
    ('f21', 'f22'), ('f22', 'f23'), ('f23', 'f24'), ('f21', 'f24'), ('f11', 'f22'), ('f16', 'f23'),
    ('f25', 'f26'), ('f26', 'f27'), ('f27', 'f28'), ('f25', 'f28'), ('f14', 'f26'), ('f16', 'f25'),
    ('f29', 'f30'), ('f30', 'f31'), ('f31', 'f32'), ('f29', 'f32'),
    ('f19', 'f30'), ('f5', 'f32')
]

# Map landmark indices to node names (f1, f2, ..., f32)
index_to_node = {index: f'f{idx+1}' for idx, index in enumerate(selected_landmark_indices)}
node_to_index = {node: idx for idx, node in enumerate(index_to_node.values())}

# Prepare data for GNN
def create_graph_data():
    all_graph_data = []

    # Iterate over all images in the folder
    for filename in tqdm(os.listdir(folder_path)):
        if filename.endswith((".JPG", ".jpeg", ".png")):
            image_path = os.path.join(folder_path, filename)

            # Read the input image
            img = cv2.imread(image_path)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            # Detect face in the image
            faces = detector(gray)

            for face in faces:
                # Detect landmarks
                landmarks = predictor(gray, face)

                # Store detected landmarks in a dictionary with selected indices
                detected_landmarks = {i: (landmarks.part(i).x, landmarks.part(i).y) for i in selected_landmark_indices if i < landmarks.num_parts}

                # Map landmark indices to node names f1, f2, ..., f32
                landmark_coords = {index_to_node[i]: detected_landmarks[i] for i in selected_landmark_indices if i in detected_landmarks}

                # Create the graph with nodes as detected landmarks
                G = nx.Graph()

                # Add nodes
                for name, point in landmark_coords.items():
                    G.add_node(name, pos=point)

                # Add edges between selected landmarks
                for (i, j) in edges:
                    if i in landmark_coords and j in landmark_coords:
                        G.add_edge(i, j)

                # Convert NetworkX graph to PyTorch Geometric format
                pos = nx.get_node_attributes(G, 'pos')
                x = torch.tensor([pos[name] for name in G.nodes()], dtype=torch.float)

                # Create edge index tensor
                edge_list = list(G.edges())
                edge_index = torch.tensor([[node_to_index[edge[0]] for edge in edge_list],
                                           [node_to_index[edge[1]] for edge in edge_list]], dtype=torch.long)

                data = Data(x=x, edge_index=edge_index)
                all_graph_data.append(data)

                # Optionally visualize the graph
                fig, axs = plt.subplots(1, 2, figsize=(12, 6))

                # Plot the graph over the image
                nx.draw(G, pos, node_color='green', node_size=70, edge_color='blue', with_labels=True, font_size=8, ax=axs[0])

                # Display the image with graph
                axs[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                axs[0].axis("off")
                axs[0].set_title('Graph Overlay')

                # Extract edge positions for plotting
                edge_positions = [(landmark_coords[i], landmark_coords[j]) for (i, j) in G.edges()]
                edge_positions = [coords for sublist in edge_positions for coords in sublist]
                edge_positions = np.array(edge_positions)

                # Plot the edge network
                axs[1].scatter(edge_positions[:, 0], edge_positions[:, 1], c='blue', s=50, label='Edges')
                for (i, j) in G.edges():
                    x = [landmark_coords[i][0], landmark_coords[j][0]]
                    y = [landmark_coords[i][1], landmark_coords[j][1]]
                    axs[1].plot(x, y, 'r-')
                axs[1].invert_yaxis()
                axs[1].set_title('Edge Network')
                axs[1].set_xlabel('X coordinate')
                axs[1].set_ylabel('Y coordinate')

                plt.tight_layout()
                # plt.show()

    return all_graph_data

# Create graph data
graph_data_list = create_graph_data()

# Now `graph_data_list` contains graph data for all images

import dlib
import cv2
import numpy as np
import networkx as nx
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
import torch
from torch_geometric.data import Data

# Initialize the dlib face detector and the facial landmark predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("/content/shape_predictor_68_face_landmarks.dat")

# Define the folder containing images
folder_path = "/content/morph/Dataset/Images/Test"

# Define the selected landmark indices
selected_landmark_indices = [0, 1, 2, 7, 8, 9, 14, 15, 16, 17, 19, 21, 22, 24, 26, 27, 30, 31, 33, 35, 36, 37, 39, 40, 42, 44, 45, 47, 48, 51, 54, 57]

# Define edges between the selected nodes using f1, f2, ..., f32
edges = [
    ('f1', 'f2'), ('f2', 'f3'), ('f3', 'f4'), ('f4', 'f5'), ('f5', 'f6'), ('f6', 'f7'), ('f7', 'f8'), ('f8', 'f9'),
    ('f10', 'f11'), ('f11', 'f12'),
    ('f13', 'f14'), ('f14', 'f15'),
    ('f16', 'f17'),
    ('f18', 'f19'), ('f19', 'f20'), ('f17', 'f19'),
    ('f21', 'f22'), ('f22', 'f23'), ('f23', 'f24'), ('f21', 'f24'), ('f11', 'f22'), ('f16', 'f23'),
    ('f25', 'f26'), ('f26', 'f27'), ('f27', 'f28'), ('f25', 'f28'), ('f14', 'f26'), ('f16', 'f25'),
    ('f29', 'f30'), ('f30', 'f31'), ('f31', 'f32'), ('f29', 'f32'),
    ('f19', 'f30'), ('f5', 'f32')
]

# Map landmark indices to node names (f1, f2, ..., f32)
index_to_node = {index: f'f{idx+1}' for idx, index in enumerate(selected_landmark_indices)}
node_to_index = {node: idx for idx, node in enumerate(index_to_node.values())}

# Prepare data for GNN
def create_graph_data(max_images=100):
    all_graph_data = []
    image_count = 0

    # Iterate over all images in the folder
    for filename in tqdm(os.listdir(folder_path)):
        if filename.endswith((".JPG", ".jpeg", ".png")):
            image_path = os.path.join(folder_path, filename)

            # Read the input image
            img = cv2.imread(image_path)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            # Detect face in the image
            faces = detector(gray)

            for face in faces:
                # Detect landmarks
                landmarks = predictor(gray, face)

                # Store detected landmarks in a dictionary with selected indices
                detected_landmarks = {i: (landmarks.part(i).x, landmarks.part(i).y) for i in selected_landmark_indices if i < landmarks.num_parts}

                # Map landmark indices to node names f1, f2, ..., f32
                landmark_coords = {index_to_node[i]: detected_landmarks[i] for i in selected_landmark_indices if i in detected_landmarks}

                # Create the graph with nodes as detected landmarks
                G = nx.Graph()

                # Add nodes
                for name, point in landmark_coords.items():
                    G.add_node(name, pos=point)

                # Add edges between selected landmarks
                for (i, j) in edges:
                    if i in landmark_coords and j in landmark_coords:
                        G.add_edge(i, j)

                # Convert NetworkX graph to PyTorch Geometric format
                pos = nx.get_node_attributes(G, 'pos')
                x = torch.tensor([pos[name] for name in G.nodes()], dtype=torch.float)

                # Create edge index tensor
                edge_list = list(G.edges())
                edge_index = torch.tensor([[node_to_index[edge[0]] for edge in edge_list],
                                           [node_to_index[edge[1]] for edge in edge_list]], dtype=torch.long)

                data = Data(x=x, edge_index=edge_index)
                all_graph_data.append(data)

                # Optionally visualize the graph
                fig, axs = plt.subplots(1, 2, figsize=(12, 6))

                # Plot the graph over the image
                nx.draw(G, pos, node_color='green', node_size=70, edge_color='blue', with_labels=True, font_size=8, ax=axs[0])

                # Display the image with graph
                axs[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
                axs[0].axis("off")
                axs[0].set_title('Graph Overlay')

                # Extract edge positions for plotting
                edge_positions = [(landmark_coords[i], landmark_coords[j]) for (i, j) in G.edges()]
                edge_positions = [coords for sublist in edge_positions for coords in sublist]
                edge_positions = np.array(edge_positions)

                # Plot the edge network
                axs[1].scatter(edge_positions[:, 0], edge_positions[:, 1], c='blue', s=50, label='Edges')
                for (i, j) in G.edges():
                    x = [landmark_coords[i][0], landmark_coords[j][0]]
                    y = [landmark_coords[i][1], landmark_coords[j][1]]
                    axs[1].plot(x, y, 'r-')
                axs[1].invert_yaxis()
                axs[1].set_title('Edge Network')
                axs[1].set_xlabel('X coordinate')
                axs[1].set_ylabel('Y coordinate')

                plt.tight_layout()
                # plt.show()

                image_count += 1
                if image_count >= max_images:
                    return all_graph_data

    return all_graph_data

# Create graph data for a maximum of 100 images
graph_data_list = create_graph_data(max_images=100)

# Now `graph_data_list` contains graph data for all images

import torch
import torch_geometric
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import DataLoader, Batch

class GNNEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GNNEncoder, self).__init__()
        self.conv1 = GCNConv(in_channels, 64)
        self.conv2 = GCNConv(64, 128)
        self.conv3 = GCNConv(128, out_channels)

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index).relu()
        x = self.conv3(x, edge_index)
        x = global_mean_pool(x, batch)  # Graph-level pooling
        return x

# Define the encoder
encoder = GNNEncoder(in_channels=2, out_channels=256)

# Use DataLoader to handle batching
data_loader = DataLoader(graph_data_list, batch_size=1, shuffle=False)  # Batch size can be adjusted

import torch

# Initialize a list to store all embeddings
all_embeddings = []

# Process each graph data in the DataLoader
for data in data_loader:
    # DataLoader already returns a batch, so you can directly access the attributes
    node_features = data.x
    edge_index = data.edge_index
    batch = data.batch

    # Get embeddings for the current batch of graphs
    embedding_vector = encoder(node_features, edge_index, batch)

    # Append to the list
    all_embeddings.append(embedding_vector)

# Convert list of embeddings to a single tensor
all_embeddings_tensor = torch.cat(all_embeddings, dim=0)

# Save to a file
torch.save(all_embeddings_tensor, '/content/embeddings.pt')

import torch
import matplotlib.pyplot as plt

# Define the FaceDecoder model
class FaceDecoder(torch.nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(FaceDecoder, self).__init__()
        self.fc1 = torch.nn.Linear(embedding_dim, 512)
        self.fc2 = torch.nn.Linear(512, 1024)
        self.fc3 = torch.nn.Linear(1024, output_dim)

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x)
        return x

# Initialize the decoder
image_size = 64 * 64 * 3
decoder = FaceDecoder(embedding_dim=256, output_dim=image_size)

# Load the saved embeddings
embeddings_path = '/content/embeddings.pt'
all_embeddings_tensor = torch.load(embeddings_path)

# Pass embeddings through the decoder
reconstructed_images_flat = decoder(all_embeddings_tensor)  # Output shape will be [batch_size, image_size]
reconstructed_images = reconstructed_images_flat.view(-1, 64, 64, 3)  # Reshape to image dimensions

# Convert tensors to NumPy arrays for visualization
reconstructed_images_np = reconstructed_images.detach().numpy()

# Optionally visualize the images using matplotlib
for i, image_np in enumerate(reconstructed_images_np):
    plt.figure(figsize=(4, 4))
    plt.imshow(image_np)
    plt.title(f'Reconstructed Image {i+1}')
    plt.axis('off')
    plt.show()

import torch
import matplotlib.pyplot as plt
import os

# Define the FaceDecoder model
class FaceDecoder(torch.nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(FaceDecoder, self).__init__()
        self.fc1 = torch.nn.Linear(embedding_dim, 512)
        self.fc2 = torch.nn.Linear(512, 1024)
        self.fc3 = torch.nn.Linear(1024, output_dim)

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x)
        return x

# Initialize the decoder
image_size = 64 * 64 * 3
decoder = FaceDecoder(embedding_dim=256, output_dim=image_size)

# Load the saved embeddings
embeddings_path = '/content/embeddings.pt'
all_embeddings_tensor = torch.load(embeddings_path)

# Pass embeddings through the decoder
reconstructed_images_flat = decoder(all_embeddings_tensor)  # Output shape will be [batch_size, image_size]
reconstructed_images = reconstructed_images_flat.view(-1, 64, 64, 3)  # Reshape to image dimensions

# Convert tensors to NumPy arrays for saving
reconstructed_images_np = reconstructed_images.detach().numpy()

# Create directory to save images if it doesn't exist
save_dir = '/content/reconstructed_images'
os.makedirs(save_dir, exist_ok=True)

# Save the images
for i, image_np in enumerate(reconstructed_images_np):
    plt.figure(figsize=(4, 4))
    plt.imshow(image_np)
    plt.axis('off')
    plt.savefig(os.path.join(save_dir, f'reconstructed_image_{i+1}.png'), bbox_inches='tight', pad_inches=0)
    plt.close()

import torch
import matplotlib.pyplot as plt
import os
import numpy as np

# Define the FaceDecoder model
class FaceDecoder(torch.nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(FaceDecoder, self).__init__()
        self.fc1 = torch.nn.Linear(embedding_dim, 512)
        self.fc2 = torch.nn.Linear(512, 1024)
        self.fc3 = torch.nn.Linear(1024, output_dim)

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x)
        return x

# Initialize the decoder
image_size = 64 * 64 * 3
decoder = FaceDecoder(embedding_dim=256, output_dim=image_size)

# Load the saved embeddings
embeddings_path = '/content/embeddings.pt'
all_embeddings_tensor = torch.load(embeddings_path)

# Pass embeddings through the decoder
reconstructed_images_flat = decoder(all_embeddings_tensor)  # Output shape will be [batch_size, image_size]
reconstructed_images = reconstructed_images_flat.view(-1, 64, 64, 3)  # Reshape to image dimensions

# Convert tensors to NumPy arrays for saving
reconstructed_images_np = reconstructed_images.detach().numpy()

# Normalize pixel values to the range [0, 1] for float data
reconstructed_images_np = np.clip(reconstructed_images_np, 0, 1)

# Create directory to save images if it doesn't exist
save_dir = '/content/reconstructed_images'
os.makedirs(save_dir, exist_ok=True)

# Save the images
for i, image_np in enumerate(reconstructed_images_np):
    plt.figure(figsize=(4, 4))
    plt.imshow(image_np)
    plt.axis('off')
    plt.savefig(os.path.join(save_dir, f'reconstructed_image_{i+1}.png'), bbox_inches='tight', pad_inches=0)
    plt.close()

import torch
import torch.nn as nn
import cv2
import numpy as np

# Define the encoder and decoder models (placeholders)
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        # Define your encoder layers here
        pass

    def forward(self, node_features, edge_index, batch=None):
        # Implement the forward pass for encoding
        pass

class Decoder(nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(Decoder, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, 512)
        self.fc2 = nn.Linear(512, 1024)
        self.fc3 = nn.Linear(1024, output_dim)

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x)
        return x

# Define the loss function
criterion = nn.MSELoss()

# Initialize models
embedding_dim = 256
image_size = 64 * 64 * 3
encoder = Encoder()
decoder = Decoder(embedding_dim=embedding_dim, output_dim=image_size)

# Load and preprocess the original image
image = cv2.imread('path_to_image.jpg')  # Replace with your image path
image = cv2.resize(image, (64, 64))
original_face = torch.tensor(image.astype(np.float32)).permute(2, 0, 1) / 255.0  # Normalize
original_face = original_face.contiguous().view(-1)  # Flatten

# Training setup
optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)

num_epochs = 10
# Example training loop
for epoch in range(num_epochs):
    optimizer.zero_grad()

    # Forward pass through encoder and decoder
    # Assuming you have node_features and edge_index as inputs for the encoder
    embedding_vector = encoder(node_features, edge_index, batch=None)
    reconstructed_face_flat = decoder(embedding_vector).view(-1)  # Flatten

    # Compute loss between original and reconstructed face
    loss = criterion(reconstructed_face_flat, original_face)

    # Backpropagation and optimization step
    loss.backward()
    optimizer.step()

    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

# Define the Encoder (Placeholder, implement according to your use case)
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        # Define your encoder layers here
        pass

    def forward(self, node_features, edge_index, batch=None):
        # Implement the forward pass for encoding
        return torch.randn(1, 256)  # Dummy output for debugging

# Define the FaceDecoder model
class FaceDecoder(nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(FaceDecoder, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, 512)
        self.fc2 = nn.Linear(512, 1024)
        self.fc3 = nn.Linear(1024, output_dim)

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x)
        return x

# Initialize models
embedding_dim = 256
image_size = 64 * 64 * 3
encoder = Encoder()
decoder = FaceDecoder(embedding_dim=embedding_dim, output_dim=image_size)

# Function to load and preprocess image
def load_and_preprocess_image(image_path):
    image = cv2.imread(image_path)
    image = cv2.resize(image, (64, 64))
    image = torch.tensor(image.astype(np.float32)).permute(2, 0, 1) / 255.0  # Normalize
    return image.contiguous().view(-1)  # Flatten

# Load morph images
morph_images_dir = '/content/morph/Dataset/Images/Train'  # Replace with your directory
morph_image_paths = [os.path.join(morph_images_dir, img_name) for img_name in os.listdir(morph_images_dir) if img_name.endswith('.JPG')]

# Shuffle and select the first 100 images
morph_image_paths = shuffle(morph_image_paths)[6:7]

# Load embeddings
embeddings_path = '/content/embeddings.pt'
all_embeddings_tensor = torch.load(embeddings_path, weights_only=True)

# Initialize loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)

num_epochs = 10
# Example training loop
for epoch in range(num_epochs):
    optimizer.zero_grad()

    for image_path in morph_image_paths:
        original_face = load_and_preprocess_image(image_path)

        # Forward pass through encoder and decoder
        embedding_vector = encoder(None, None, None)  # Replace with actual inputs
        reconstructed_face_flat = decoder(embedding_vector).view(-1)  # Flatten

        # Compute loss between original and reconstructed face
        loss = criterion(reconstructed_face_flat, original_face)

        # Backpropagation and optimization step
        loss.backward()
        optimizer.step()

        # Print debugging information
        print(f'Epoch {epoch+1}, Image {os.path.basename(image_path)}, Loss: {loss.item()}')

    # Optional: Save reconstructed images at intervals
    if (epoch + 1) % 5 == 0:
        with torch.no_grad():
            # Generate and save a batch of images
            embedding_vector = encoder(None, None, None)  # Replace with actual inputs
            reconstructed_images = decoder(embedding_vector).view(-1, 64, 64, 3)
            reconstructed_images_np = np.clip(reconstructed_images.numpy(), 0, 1) * 255
            save_dir = '/content/reconstructed_ima'
            os.makedirs(save_dir, exist_ok=True)
            for i, image_np in enumerate(reconstructed_images_np):
                image_np = image_np.astype(np.uint8)
                plt.imsave(os.path.join(save_dir, f'reconstructed_image_epoch_{epoch+1}_{i+1}.png'), image_np)

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

# Define the FaceDecoder model
class FaceDecoder(nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(FaceDecoder, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, 512)
        self.fc2 = nn.Linear(512, 1024)
        self.fc3 = nn.Linear(1024, output_dim)

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x)
        return x

# Initialize models
embedding_dim = 256
image_size = 64 * 64 * 3
decoder = FaceDecoder(embedding_dim=embedding_dim, output_dim=image_size)

# Load embeddings
embeddings_path = '/content/embeddings.pt'
all_embeddings_tensor = torch.load(embeddings_path)  # Load the embeddings

# Load morph images
morph_images_dir = '/content/morph/Dataset/Images/Train'  # Replace with your directory
morph_image_paths = [os.path.join(morph_images_dir, img_name) for img_name in os.listdir(morph_images_dir) if img_name.endswith('.JPG')]

# Shuffle and select the first 100 images
morph_image_paths = shuffle(morph_image_paths)[:1]

# Initialize loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(decoder.parameters(), lr=0.001)

num_epochs = 40
# Training loop
for epoch in range(num_epochs):
    optimizer.zero_grad()

    for idx, image_path in enumerate(morph_image_paths):
        original_face = load_and_preprocess_image(image_path)

        # Use the corresponding embedding from the loaded embeddings
        embedding_vector = all_embeddings_tensor[idx].unsqueeze(0)  # Get the corresponding embedding

        # Forward pass through the decoder
        reconstructed_face_flat = decoder(embedding_vector).view(-1)  # Flatten

        # Compute loss between original and reconstructed face
        loss = criterion(reconstructed_face_flat, original_face)

        # Backpropagation and optimization step
        loss.backward()
        optimizer.step()

        # Print debugging information
        print(f'Epoch {epoch+1}, Image {os.path.basename(image_path)}, Loss: {loss.item()}')

    # Optional: Save reconstructed images at intervals
    if (epoch + 1) % 5 == 0:
        with torch.no_grad():
            # Generate and save a batch of images
            reconstructed_images = decoder(embedding_vector).view(-1, 64, 64, 3)
            reconstructed_images_np = np.clip(reconstructed_images.numpy(), 0, 1) * 255
            save_dir = '/content/reconstructed_images'
            os.makedirs(save_dir, exist_ok=True)
            for i, image_np in enumerate(reconstructed_images_np):
                image_np = image_np.astype(np.uint8)
                plt.imsave(os.path.join(save_dir, f'reconstructed_image_epoch_{epoch+1}_{i+1}.png'), image_np)

# Visualization of the final image
reconstructed_face_image = reconstructed_face_flat.view(3, 64, 64).detach().numpy().transpose(1, 2, 0) * 255.0
reconstructed_face_image = np.clip(reconstructed_face_image, 0, 255).astype(np.uint8)

original_face_image = original_face.view(3, 64, 64).detach().numpy().transpose(1, 2, 0) * 255.0
original_face_image = np.clip(original_face_image, 0, 255).astype(np.uint8)

plt.figure(figsize=(10, 5))

# Original face image
plt.subplot(1, 2, 1)
plt.imshow(original_face_image)
plt.title("Original Face Image")
plt.axis("off")

# Reconstructed face image
plt.subplot(1, 2, 2)
plt.imshow(reconstructed_face_image)
plt.title("Reconstructed Face Image")
plt.axis("off")

plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from scipy.ndimage import median_filter

# Define the FaceDecoder model with an additional layer for complexity
# Define the FaceDecoder model with an additional layer for complexity
class FaceDecoder(nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(FaceDecoder, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, 512)
        self.fc2 = nn.Linear(512, 1024)
        self.fc3 = nn.Linear(1024, 2048)
        self.fc4 = nn.Linear(2048, output_dim)  # Output layer

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x).relu()
        x = self.fc4(x)
        return x


# Initialize models
embedding_dim = 256
image_size = 64 * 64 * 3
decoder = FaceDecoder(embedding_dim=embedding_dim, output_dim=image_size)

# Load embeddings
embeddings_path = '/content/embeddings.pt'
all_embeddings_tensor = torch.load(embeddings_path)  # Load the embeddings

# Load morph images
morph_images_dir = '/content/morph/Dataset/Images/Train'  # Replace with your directory
morph_image_paths = [os.path.join(morph_images_dir, img_name) for img_name in os.listdir(morph_images_dir) if img_name.endswith('.JPG')]

# Shuffle and select the first 100 images
morph_image_paths = shuffle(morph_image_paths)[:]

# Initialize loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(decoder.parameters(), lr=0.008, weight_decay=1e-5)  # Lower learning rate, with weight decay

num_epochs = 40
# Training loop
for epoch in range(num_epochs):
    for idx, image_path in enumerate(morph_image_paths):
        optimizer.zero_grad()

        original_face = load_and_preprocess_image(image_path)

        # Use the corresponding embedding from the loaded embeddings
        embedding_vector = all_embeddings_tensor[idx].unsqueeze(0)  # Get the corresponding embedding

        # Forward pass through the decoder
        reconstructed_face_flat = decoder(embedding_vector).view(-1)  # Flatten

        # Compute loss between original and reconstructed face
        loss = criterion(reconstructed_face_flat, original_face)

        # Backpropagation and optimization step
        loss.backward()
        optimizer.step()

        # Print debugging information
        print(f'Epoch {epoch+1}, Image {os.path.basename(image_path)}, Loss: {loss.item()}')

    # Optional: Save reconstructed images at intervals
    if (epoch + 1) % 5 == 0:
        with torch.no_grad():
            # Generate and save a batch of images
            reconstructed_images = decoder(embedding_vector).view(-1, 64, 64, 3)
            reconstructed_images_np = np.clip(reconstructed_images.numpy(), 0, 1) * 255
            save_dir = '/content/reconstructed_images'
            os.makedirs(save_dir, exist_ok=True)
            for i, image_np in enumerate(reconstructed_images_np):
                image_np = image_np.astype(np.uint8)
                plt.imsave(os.path.join(save_dir, f'reconstructed_image_epoch_{epoch+1}_{i+1}.png'), image_np)

# Visualization of the final image with denoising
reconstructed_face_image = reconstructed_face_flat.view(3, 64, 64).detach().numpy().transpose(1, 2, 0) * 255.0
reconstructed_face_image = np.clip(reconstructed_face_image, 0, 255).astype(np.uint8)

# Apply median filter for denoising
reconstructed_face_image = median_filter(reconstructed_face_image, size=3)

original_face_image = original_face.view(3, 64, 64).detach().numpy().transpose(1, 2, 0) * 255.0
original_face_image = np.clip(original_face_image, 0, 255).astype(np.uint8)

plt.figure(figsize=(10, 5))

# Original face image
plt.subplot(1, 2, 1)
plt.imshow(original_face_image)
plt.title("Original Face Image")
plt.axis("off")

# Reconstructed face image
plt.subplot(1, 2, 2)
plt.imshow(reconstructed_face_image)
plt.title("Reconstructed Face Image")
plt.axis("off")

plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from scipy.ndimage import median_filter

# Define the FaceDecoder model with an additional layer for complexity
class FaceDecoder(nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(FaceDecoder, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, 512)
        self.fc2 = nn.Linear(512, 1024)
        self.fc3 = nn.Linear(1024, 2048)
        self.fc4 = nn.Linear(2048, output_dim)

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x).relu()
        x = self.fc4(x)
        return x

# Initialize models
embedding_dim = 256
image_size = 64 * 64 * 3
decoder = FaceDecoder(embedding_dim=embedding_dim, output_dim=image_size)

# Load embeddings
embeddings_path = '/content/embeddings.pt'
all_embeddings_tensor = torch.load(embeddings_path)  # Load the embeddings

# Load morph images
morph_images_dir = '/content/morph/Dataset/Images/Train'  # Replace with your directory
morph_image_paths = [os.path.join(morph_images_dir, img_name) for img_name in os.listdir(morph_images_dir) if img_name.endswith('.JPG')]

# Shuffle and select a subset of images for sampling
sample_size = 1  # Number of images to sample
morph_image_paths = shuffle(morph_image_paths)[:sample_size]

# Initialize loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(decoder.parameters(), lr=0.008, weight_decay=1e-5)  # Lower learning rate, with weight decay

num_epochs = 6
# Training loop
for epoch in range(num_epochs):
    for idx, image_path in enumerate(morph_image_paths):
        optimizer.zero_grad()

        original_face = load_and_preprocess_image(image_path)

        # Use the corresponding embedding from the loaded embeddings
        embedding_vector = all_embeddings_tensor[idx].unsqueeze(0)  # Get the corresponding embedding

        # Forward pass through the decoder
        reconstructed_face_flat = decoder(embedding_vector).view(-1)  # Flatten

        # Compute loss between original and reconstructed face
        loss = criterion(reconstructed_face_flat, original_face)

        # Backpropagation and optimization step
        loss.backward()
        optimizer.step()

        # Print debugging information
        print(f'Epoch {epoch+1}, Image {os.path.basename(image_path)}, Loss: {loss.item()}')

    # Optional: Save reconstructed images at intervals
    if (epoch + 1) % 5 == 0:
        with torch.no_grad():
            # Generate and save a batch of images
            reconstructed_images = decoder(embedding_vector).view(-1, 64, 64, 3)
            reconstructed_images_np = np.clip(reconstructed_images.numpy(), 0, 1) * 255
            save_dir = '/content/reconstructed_images'
            os.makedirs(save_dir, exist_ok=True)
            for i, image_np in enumerate(reconstructed_images_np):
                image_np = image_np.astype(np.uint8)
                plt.imsave(os.path.join(save_dir, f'reconstructed_image_epoch_{epoch+1}_{i+1}.png'), image_np)

# Visualization of the final image with denoising
reconstructed_face_image = reconstructed_face_flat.view(3, 64, 64).detach().numpy().transpose(1, 2, 0) * 255.0
reconstructed_face_image = np.clip(reconstructed_face_image, 0, 255).astype(np.uint8)

# Apply median filter for denoising
reconstructed_face_image = median_filter(reconstructed_face_image, size=3)

original_face_image = original_face.view(3, 64, 64).detach().numpy().transpose(1, 2, 0) * 255.0
original_face_image = np.clip(original_face_image, 0, 255).astype(np.uint8)

plt.figure(figsize=(10, 5))

# Original face image
plt.subplot(1, 2, 1)
plt.imshow(original_face_image)
plt.title("Original Face Image")
plt.axis("off")

# Reconstructed face image
plt.subplot(1, 2, 2)
plt.imshow(reconstructed_face_image)
plt.title("Reconstructed Face Image")
plt.axis("off")

plt.show()

class FaceDecoder(nn.Module):
    def __init__(self, embedding_dim, output_dim):
        super(FaceDecoder, self).__init__()
        self.fc1 = nn.Linear(embedding_dim, 512)
        self.fc2 = nn.Linear(512, 1024)
        self.fc3 = nn.Linear(1024, 2048)
        self.fc4 = nn.Linear(2048, output_dim)  # Update this to output 128x128x3

    def forward(self, embedding):
        x = self.fc1(embedding).relu()
        x = self.fc2(x).relu()
        x = self.fc3(x).relu()
        x = self.fc4(x)
        return x

def load_and_preprocess_image(image_path):
    image = cv2.imread(image_path)
    image = cv2.resize(image, (128, 128))  # Resize to 128x128
    image = image.astype(np.float32) / 255.0  # Normalize to [0, 1]
    image = torch.tensor(image).permute(2, 0, 1)  # Convert to tensor and reorder dimensions
    return image

# Ensure image_size is 128*128*3
image_size = 128 * 128 * 3

# Update decoder initialization
decoder = FaceDecoder(embedding_dim=embedding_dim, output_dim=image_size)

# Training loop
for epoch in range(num_epochs):
    for idx, image_path in enumerate(morph_image_paths):
        optimizer.zero_grad()

        original_face = load_and_preprocess_image(image_path).reshape(-1)  # Flatten the original image tensor

        # Use the corresponding embedding from the loaded embeddings
        embedding_vector = all_embeddings_tensor[idx].unsqueeze(0)  # Get the corresponding embedding

        # Forward pass through the decoder
        reconstructed_face_flat = decoder(embedding_vector).reshape(-1)  # Flatten the reconstructed image tensor

        # Compute loss between original and reconstructed face
        loss = criterion(reconstructed_face_flat, original_face)

        # Backpropagation and optimization step
        loss.backward()
        optimizer.step()

        # Print debugging information
        print(f'Epoch {epoch+1}, Image {os.path.basename(image_path)}, Loss: {loss.item()}')

    # Optional: Save reconstructed images at intervals
    if (epoch + 1) % 5 == 0:
        with torch.no_grad():
            # Generate and save a batch of images
            reconstructed_images = decoder(embedding_vector).reshape(-1, 128, 128, 3)  # Reshape to 128x128x3
            reconstructed_images_np = np.clip(reconstructed_images.numpy(), 0, 1) * 255
            save_dir = '/content/reconstructed_images'
            os.makedirs(save_dir, exist_ok=True)
            for i, image_np in enumerate(reconstructed_images_np):
                image_np = image_np.astype(np.uint8)
                plt.imsave(os.path.join(save_dir, f'reconstructed_image_epoch_{epoch+1}_{i+1}.png'), image_np)

# Visualization of the final image with denoising
reconstructed_face_image = reconstructed_face_flat.reshape(3, 128, 128).detach().numpy().transpose(1, 2, 0) * 255.0
reconstructed_face_image = np.clip(reconstructed_face_image, 0, 255).astype(np.uint8)

# Apply median filter for denoising
reconstructed_face_image = median_filter(reconstructed_face_image, size=3)

original_face_image = original_face.reshape(3, 128, 128).detach().numpy().transpose(1, 2, 0) * 255.0
original_face_image = np.clip(original_face_image, 0, 255).astype(np.uint8)

plt.figure(figsize=(10, 5))

# Original face image
plt.subplot(1, 2, 1)
plt.imshow(original_face_image)
plt.title("Original Face Image")
plt.axis("off")

# Reconstructed face image
plt.subplot(1, 2, 2)
plt.imshow(reconstructed_face_image)
plt.title("Reconstructed Face Image")
plt.axis("off")

plt.show()